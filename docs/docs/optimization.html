<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>PX 915: Variational Quantum Monte Carlo - LATIN: Optimisation for a large parameter search-space</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">PX 915: Variational Quantum Monte Carlo - LATIN
   &#160;<span id="projectnumber">GroupB</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',false,false,'search.php','Search');
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('optimization.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Optimisation for a large parameter search-space </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="intro"></a>
Gaussian Process</h1>
<p>Given that MCMC evaluations for a given set of points in parameter space can quickly become expensive, we are motivated to use a surrogate model that can be used when searching for the best possible set of parameters to trial. Here we present an approach, as detailed by Wang et al <a class="el" href="citelist.html#CITEREF_wang2019">[8]</a>, used to perform a Bayesian global optimisation.</p>
<p>We start by defining our prior distribution on our ground state calculation (as found using MCMC) as a Gaussian process:</p>
<p class="formulaDsp">
\begin{equation}\tag {1} f(\boldsymbol{X}) \sim \mathcal{G} \mathcal{P} (\boldsymbol{\mu}^{(0)}, \boldsymbol{\Sigma}^{(0)}) \end{equation}
</p>
<p>Where \( \boldsymbol{X} \) is of set of points in parameter space, \( \boldsymbol{\mu} \) is our mean function which is represented as a constant, and \( \boldsymbol{\Sigma} \) defines our covariance matrix which is induced by a Gaussian kernel. The posterior mean and covariance matrix are updated for a set of new points \( \boldsymbol{x}^{(1:n)} \) via:</p>
<p class="formulaDsp">
\begin{equation}\tag {2} \boldsymbol{\mu}^{(n)} = \boldsymbol{\mu}^{(0)} + K(\boldsymbol{X}, \boldsymbol{x}^{(1:n)} ) K(\boldsymbol{x}^{(1:n)},\boldsymbol{x}^{(1:n)})^{-1} (y^{(1:n)} - \mu (\boldsymbol{x}^{(1:n)})) \end{equation}
</p>
<p class="formulaDsp">
\begin{equation}\tag {3} \boldsymbol{\Sigma}^{(n)} = K(\boldsymbol{X},\boldsymbol{X}) - K(\boldsymbol{X}, \boldsymbol{x}^{(1:n)} ) K(\boldsymbol{x}^{(1:n)}, \boldsymbol{x}^{(1:n)} )^{-1} K(\boldsymbol{x}^{(1:n)},\boldsymbol{X} ) \end{equation}
</p>
<p> Where \( y^{(1:n)} \) is the value of the function being optimised at a given set of parameters, \( \mu (\boldsymbol{x}^{(1:n)}) \) is obtained from the prior mean function across the new set, and the function K is computing the covariance (also Gaussian kernel) between each entry in \( \boldsymbol{X} \) and \( \boldsymbol{x}^{(1:n)} \).</p>
<h1><a class="anchor" id="grad_dec"></a>
Optimisation of parameters</h1>
<p>Given we have constructed a surrogate model we may evaluate a large set of test points through the expected improvement q-EI:</p>
<p class="formulaDsp">
\begin{equation}\tag {4} q\textrm{-EI} = \mathop{\mathbb{E}} [ \max_{i=0,... q} \boldsymbol{e}_i [\boldsymbol{m}(\boldsymbol{X}) + \boldsymbol{C}(\boldsymbol{X})\boldsymbol{Z}]] \end{equation}
</p>
<p>Where \( \boldsymbol{e}_i \) is a unit vector which acts as a selector across the other vectors \( \boldsymbol{m}(\boldsymbol{X}) \) and \( \boldsymbol{C}(\boldsymbol{X})\boldsymbol{Z} \). The vector \( \boldsymbol{m}(\boldsymbol{X}) \) evaluates the difference between the previous best result and the posterior mean, \( \boldsymbol{Z} \) is a vector containing samples from a standard normal random vector, and \( \boldsymbol{C}(\boldsymbol{X}) \) contains the negative of the Cholesky decomposition of posterior covariance matrix. For the zeroth thread (i=0) both \( \boldsymbol{m}(\boldsymbol{X}) \) and \( \boldsymbol{C}(\boldsymbol{X}) \) are returned as 0. The stochastic gradient estimator of the expected improvement can then be constructed as:</p>
<p class="formulaDsp">
\begin{equation}\tag {5} \boldsymbol{g}(\boldsymbol{X}, \boldsymbol{Z}) = \begin{cases} \nabla h (\boldsymbol{X}, \boldsymbol{Z}) ,&amp; \text{if } \nabla h (\boldsymbol{X}, \boldsymbol{Z}) \text{ exists} \\ 0, &amp; \text{otherwise} \end{cases} \end{equation}
</p>
<p>where:</p>
<p class="formulaDsp">
\begin{equation}\tag {6} h(\boldsymbol{X}, \boldsymbol{Z}) = \max_{i=0,... q} \boldsymbol{e}_i [\boldsymbol{m}(\boldsymbol{X}) + \boldsymbol{C}(\boldsymbol{X})\boldsymbol{Z}] \end{equation}
</p>
<p> Following the algorithms outlined by S. P. Smith <a class="el" href="citelist.html#CITEREF_smith1995">[7]</a> on the backward differentiation of Cholesky dependent functions one may find the derivatives to \( \boldsymbol{C}(\boldsymbol{X}) \) in the above with respect to each parameter. Differentiation over each parameter also needs to be computed for \( \boldsymbol{m}(\boldsymbol{X}) \) seperately and added into get the stochastic gradient estimator. From this we may evaluate a gradient estimate at point \( \boldsymbol{X}_t \) as:</p>
<p class="formulaDsp">
\begin{equation}\tag {7} \boldsymbol{G}( \boldsymbol{X}_t) = \frac{1}{M} \sum_{m=1}^{M} \boldsymbol{g}(\boldsymbol{X}_t, \boldsymbol{Z}_{t,m}) \end{equation}
</p>
<p>Where \( M \) is a number of samples the stochastic gradient estimator is averaged over. From this we generate the next set of points in parameter space to test as:</p>
<p class="formulaDsp">
\begin{equation}\tag {8} \boldsymbol{X}_{t+1} = \underset{H}{\Pi}[\boldsymbol{X}_{t} + \epsilon_t \boldsymbol{G}( \boldsymbol{X}_t) ] \end{equation}
</p>
<p> Where \( \underset{H}{\Pi} \) defines the projection (back) into the allowed parameter space and \( \epsilon_t \) defines a step size for the points in parameter to space to wander. The step size decrease as the simulation goes on. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Thu May 27 2021 03:14:24 for PX 915: Variational Quantum Monte Carlo - LATIN by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
